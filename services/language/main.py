# akshu-ai/services/language/main.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union

# Import necessary libraries for LLMs and embeddings
# These imports assume you have the required libraries installed (e.g., transformers, torch, sentence-transformers)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from sentence_transformers import SentenceTransformer
    import torch
    # TODO: Add logic to check for GPU availability and set the device accordingly
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {DEVICE}")
except ImportError as e:
    print(f"Error importing AI libraries: {e}")
    print("Please ensure transformers, torch/tensorflow, and sentence-transformers are installed.")
    # Define placeholder classes/functions if imports fail, or raise a critical error
    AutoModelForCausalLM = None
    AutoTokenizer = None
    SentenceTransformer = None
    torch = None
    DEVICE = "cpu"


app = FastAPI()

# --- Configuration and Model Loading ---

# Placeholder for loaded models and tokenizers
# TODO: Implement a more robust model management system:
# - Load multiple models (different sizes, types)
# - Lazy loading or dynamic switching based on request/task
# - Handle model parallelism or distribution for large models
# - Load model configurations from a central config service or file

language_model = None # For text generation (e.g., LLaMA, Phi)
language_tokenizer = None

embedding_model = None # For generating text embeddings

# TODO: Make model names configurable
DEFAULT_LANGUAGE_MODEL_NAME = "gpt2" # A small model for basic testing
DEFAULT_EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" # A common embedding model

def load_language_model(model_name: str = DEFAULT_LANGUAGE_MODEL_NAME):
    """Loads a language model and tokenizer."""
    global language_model, language_tokenizer
    if AutoModelForCausalLM is None or AutoTokenizer is None:
        print("AI libraries not available, cannot load language model.")
        return
    try:
        print(f"Loading language model: {model_name}...")
        language_tokenizer = AutoTokenizer.from_pretrained(model_name)
        language_model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)
        language_model.eval() # Set model to evaluation mode
        print(f"Successfully loaded language model: {model_name}")
    except Exception as e:
        print(f"Error loading language model {model_name}: {e}")
        # TODO: Implement proper error handling (e.g., retry, alert)
        language_model = None
        language_tokenizer = None

def load_embedding_model(model_name: str = DEFAULT_EMBEDDING_MODEL_NAME):
    """Loads an embedding model."""
    global embedding_model
    if SentenceTransformer is None:
        print("SentenceTransformer library not available, cannot load embedding model.")
        return
    try:
        print(f"Loading embedding model: {model_name}...")
        embedding_model = SentenceTransformer(model_name, device=DEVICE)
        print(f"Successfully loaded embedding model: {model_name}")
    except Exception as e:
        print(f"Error loading embedding model {model_name}: {e}")
        # TODO: Implement proper error handling
        embedding_model = None

# Load models on startup
# TODO: Implement lazy loading or background loading
load_language_model()
load_embedding_model()

# --- Pydantic Models for API ----

class GenerateRequest(BaseModel):
    prompt: str = Field(..., description="The input text prompt for the language model.")
    max_tokens: int = Field(150, description="The maximum number of tokens to generate.", ge=1, le=2048) # Example limits
    temperature: float = Field(0.7, description="Controls randomness in generation. Lower values make output more deterministic.", ge=0.0, le=2.0)
    top_p: float = Field(1.0, description="Nucleus sampling. Only the most likely tokens with probabilities adding up to top_p are kept.", ge=0.0, le=1.0)
    # TODO: Add more generation parameters as needed (e.g., num_beams, do_sample, repetition_penalty)
    stop_sequences: Optional[List[str]] = Field(None, description="Sequences that stop the generation.")
    user_id: Optional[str] = Field(None, description="Optional user ID for personalization or logging.")
    session_id: Optional[str] = Field(None, description="Optional session ID for conversational context.")
    # TODO: Add context field for RAG or conversation history
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context for the LLM (e.g., retrieved documents from memory).")

class GenerateResponse(BaseModel):
    generated_text: str = Field(..., description="The text generated by the language model.")
    # TODO: Add more response details (e.g., finish reason, token usage)
    finish_reason: str = Field("completed", description="The reason the generation finished (e.g., 'max_tokens', 'stop_sequence').")

class EmbedRequest(BaseModel):
    texts: Union[List[str], str] = Field(..., description="A single string or a list of strings to embed.")
    # TODO: Add model name selection if multiple embedding models are supported

class EmbedResponse(BaseModel):
    embeddings: List[List[float]] = Field(..., description="A list of embeddings, where each embedding is a list of floats.")
    model: str = Field(..., description="The name of the embedding model used.")

class TokenizeRequest(BaseModel):
    text: str = Field(..., description="The text to tokenize.")

class TokenizeResponse(BaseModel):
    tokens: List[int] = Field(..., description="A list of token IDs.")
    n_tokens: int = Field(..., description="The number of tokens.")

# --- API Endpoints ---

@app.get("/")
def read_root():
    return {"message": "Language service is running"}

@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """Generates text based on a prompt using the loaded language model."""
    if language_model is None or language_tokenizer is None:
        raise HTTPException(status_code=503, detail="Language model is not loaded.")

    # TODO: Implement sophisticated prompting logic:
    # - Incorporate conversation history (retrieved from Memory service via Orchestrator)
    # - Integrate retrieved context (RAG) into the prompt
    # - Format the prompt based on the model's requirements
    # - Handle different prompt templates or styles (e.g., chat, instruction)

    # Simple prompt for now
    prompt_text = request.prompt
    if request.context:
        # Basic example of adding context (needs refinement based on context structure and model)
        context_string = "
".join([f"{k}: {v}" for k, v in request.context.items()])
        prompt_text = f"Context: {context_string}

{request.prompt}"
        print(f"Prompt with context: {prompt_text}")

    try:
        # TODO: Implement robust tokenization, handling of long inputs (truncation, splitting)
        input_ids = language_tokenizer.encode(prompt_text, return_tensors="pt").to(DEVICE)

        # TODO: Implement sophisticated generation parameters and strategies:
        # - Beam search, sampling, top-k, top-p, etc.
        # - Handle stop sequences correctly during generation
        # - Manage attention masks and past_key_values for efficiency in conversational settings
        # - Implement output filtering or moderation if necessary

        # Basic generation call
        output = language_model.generate(
            input_ids,
            max_length=request.max_tokens + len(input_ids[0]), # max_length includes input tokens
            temperature=request.temperature,
            top_p=request.top_p,
            num_return_sequences=1,
            pad_token_id=language_tokenizer.eos_token_id, # Or a specific pad token if defined
            # TODO: Add stop sequence handling during generation
        )

        # TODO: Implement robust decoding, handling special tokens
        generated_text = language_tokenizer.decode(output[0][len(input_ids[0]):], skip_special_tokens=True)

        # Determine finish reason (basic placeholder)
        finish_reason = "max_tokens" if len(output[0]) >= request.max_tokens + len(input_ids[0]) else "completed"
        if request.stop_sequences and any(seq in generated_text for seq in request.stop_sequences):
             finish_reason = "stop_sequence"
             # TODO: Truncate text after stop sequence if necessary

        return GenerateResponse(generated_text=generated_text, finish_reason=finish_reason)

    except Exception as e:
        print(f"Error during text generation: {e}")
        # TODO: Implement proper error logging and potentially re-raise a specific exception
        raise HTTPException(status_code=500, detail=f"Error during text generation: {e}")

@app.post("/embed", response_model=EmbedResponse)
async def embed_text(request: EmbedRequest):
    """Generates embeddings for the given text(s) using the loaded embedding model."""
    if embedding_model is None:
         raise HTTPException(status_code=503, detail="Embedding model is not loaded.")

    # Ensure texts is a list
    texts_list = [request.texts] if isinstance(request.texts, str) else request.texts

    if not texts_list:
        return EmbedResponse(embeddings=[], model=DEFAULT_EMBEDDING_MODEL_NAME)

    try:
        # TODO: Implement batch processing for efficiency, especially for large lists of texts
        # TODO: Handle potential input text cleaning or preprocessing before embedding

        # Generate embeddings
        # The encode method handles tokenization and pooling internally
        embeddings = embedding_model.encode(texts_list, convert_to_list=True)

        return EmbedResponse(embeddings=embeddings, model=DEFAULT_EMBEDDING_MODEL_NAME)

    except Exception as e:
        print(f"Error during text embedding: {e}")
        # TODO: Implement proper error logging
        raise HTTPException(status_code=500, detail=f"Error during text embedding: {e}")

@app.post("/tokenize", response_model=TokenizeResponse)
async def tokenize_text(request: TokenizeRequest):
    """Tokenizes the given text using the language model's tokenizer.""" sponsorships
    if language_tokenizer is None:
        raise HTTPException(status_code=503, detail="Language tokenizer is not loaded.")

    try:
        # TODO: Handle different tokenization options (e.g., add special tokens)
        tokens = language_tokenizer.encode(request.text)
        return TokenizeResponse(tokens=tokens, n_tokens=len(tokens))
    except Exception as e:
        print(f"Error during tokenization: {e}")
        raise HTTPException(status_code=500, detail=f"Error during tokenization: {e}")

# TODO: Add more endpoints for other NLP tasks (e.g., summarization, translation,
# sentiment analysis, named entity recognition) as defined by the architecture.
# Each endpoint should have appropriate Pydantic models and detailed internal logic.
